{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D \n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "from tensorflow.keras import backend as k\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(paths, verbose=-1):\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    # Loop over input images\n",
    "    for (i,imgpath) in enumerate(paths):\n",
    "        #  load image and extract label\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.array(im_gray).flatten()\n",
    "        label = imgpath.split(os.path.sep)[-2]\n",
    "        #  scale image\n",
    "        data.append(image/255)\n",
    "        labels.append(label)\n",
    "        #  show update\n",
    "        if verbose > 0 and i > 0 and (i+1) % verbose ==  0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
    "        #  return tuple mof data and label\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/60000\n",
      "[INFO] processed 20000/60000\n",
      "[INFO] processed 30000/60000\n",
      "[INFO] processed 40000/60000\n",
      "[INFO] processed 50000/60000\n",
      "[INFO] processed 60000/60000\n"
     ]
    }
   ],
   "source": [
    "#  declare path\n",
    "# img_path = '/home/nextg3/Documents/FederatedLearning/FL_final/MNIST_training'\n",
    "img_path = '/home/nextg3/Documents/Thesis/Code/FL/MNIST_training'\n",
    "# Get path list\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "#  apply function\n",
    "image_list, label_list = load(image_paths, verbose = 10000)\n",
    "#  Binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n",
    "# split train test\n",
    "X_train,X_test, y_train, y_test = train_test_split(image_list,\n",
    "                                                   label_list,\n",
    "                                                   test_size=0.1,\n",
    "                                                   random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
    "    '''returns a dictionary with keys as client names and values as data shards - (image, label)\n",
    "        args:\n",
    "    \n",
    "                '''\n",
    "    #  create list of client names\n",
    "    client_names = ['{}_{}'.format(initial,i) for i in range(num_clients)]\n",
    "    #  randomize data\n",
    "    data = list(zip(image_list,label_list))\n",
    "    random.shuffle(data)\n",
    "    #  place data at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i+size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    # no of shrads should equa no of clients\n",
    "    assert(len(shards)==len(client_names))\n",
    "\n",
    "    return {client_names[i]:shards[i] for i in range(len(client_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create clients\n",
    "clients = create_clients(X_train,y_train,num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''client data shard -> tfds object\n",
    "        args:\n",
    "            shard\n",
    "            batch size\n",
    "        return\n",
    "            tfds object'''\n",
    "    #  sep data and label\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data),list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and batch data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "#  Processed and batch the test set\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "coms_round = 100\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(learning_rate = lr,\n",
    "                momentum = 0.9\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scaling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    # get bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    # print('bs',bs)\n",
    "    # Calculater the total data points across the clients\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # print('glocal_count: ',global_count)\n",
    "    # get total no of data points held by the client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    # print('local_count: ',local_count)\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''Function for scaling a model's weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar*weight[i])\n",
    "    return weight_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  sum_scaled_weights(scaled_weight_list):\n",
    "    '''Returns the sum of the listed scaled weights.\n",
    "    scaled average of the weights'''\n",
    "    avg_grad = list()\n",
    "    #  get the average grad over all client gradients\n",
    "    for grad_list_tupel in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tupel, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test, model, coms_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    # logits=model.predict(X_test, batch_size=100)\n",
    "    logits=model.predict(X_test)\n",
    "    loss = cce(y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis  = 1), tf.argmax(Y_test, axis = 1))\n",
    "    print('coom_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 637us/step\n",
      "coom_round: 0 | global_acc: 90.433% | global_loss: 1.6292821168899536\n",
      "188/188 [==============================] - 0s 778us/step\n",
      "coom_round: 1 | global_acc: 92.450% | global_loss: 1.5880060195922852\n",
      "188/188 [==============================] - 0s 610us/step\n",
      "coom_round: 2 | global_acc: 93.283% | global_loss: 1.5702006816864014\n",
      "188/188 [==============================] - 0s 611us/step\n",
      "coom_round: 3 | global_acc: 94.500% | global_loss: 1.5560517311096191\n",
      "188/188 [==============================] - 0s 692us/step\n",
      "coom_round: 4 | global_acc: 94.783% | global_loss: 1.5468307733535767\n",
      "188/188 [==============================] - 0s 647us/step\n",
      "coom_round: 5 | global_acc: 95.183% | global_loss: 1.5397067070007324\n",
      "188/188 [==============================] - 0s 608us/step\n",
      "coom_round: 6 | global_acc: 95.633% | global_loss: 1.5326088666915894\n",
      "188/188 [==============================] - 0s 634us/step\n",
      "coom_round: 7 | global_acc: 95.717% | global_loss: 1.5278239250183105\n",
      "188/188 [==============================] - 0s 599us/step\n",
      "coom_round: 8 | global_acc: 95.967% | global_loss: 1.5244638919830322\n",
      "188/188 [==============================] - 0s 588us/step\n",
      "coom_round: 9 | global_acc: 96.050% | global_loss: 1.520694375038147\n"
     ]
    }
   ],
   "source": [
    "# Initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784,10)\n",
    "\n",
    "comms_round = 10\n",
    "\n",
    "# start global training loop\n",
    "for comm_round in range(comms_round):\n",
    "    #  get the global model's weights - initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "\n",
    "    # List to collect local model weights after scaling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    # Randomize client data using keys\n",
    "    client_names = list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "\n",
    "    # Loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784,10)\n",
    "        local_model.compile(loss=loss,\n",
    "                            optimizer=optimizer,\n",
    "                            metrics=metrics)\n",
    "        # set weights of the global model as weights of the local modelk\n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        # Fit local model with client data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "\n",
    "        # scale the model weights and add to the list\n",
    "        # scaling_factor = weight_scaling_factor(clients_batched, client)\n",
    "        # scaled_weights = scale_model_weights(local_model.get_weights(),scaling_factor)\n",
    "        # scaled_local_weightsmlp_local = SimpleMLP()\n",
    "        # local_model = smlp_local.build(784,10)\n",
    "        # local_model.compile(loss=loss,\n",
    "        #                     optimizer=optimizer,\n",
    "        #                     metrics=metrics)\n",
    "        # # set weights of the global model as weights of the local modelk\n",
    "        # local_model.set_weights(global_weights)\n",
    "\n",
    "        # # Fit local model with client data\n",
    "        # local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "\n",
    "        # scale the model weights and add to the list\n",
    "        scaling_factor = weight_scaling_factor(clients_batched, client)\n",
    "        # save loca_weights\n",
    "        local_model.save_weights('{}_weights.h5'.format(client))\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(),scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "        # Clear session to clean memory\n",
    "        k.clear_session()\n",
    "    \n",
    "    # Get the average overall model\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "    # update global model\n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    # Test globalmodel\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[<tf.Tensor: shape=(784, 200), dtype=float32, numpy=\\narray([[-0.05319114, -0.019837  ,  0.06939308, ..., -0.04295979,\\n        -0.06557755,  0.04630484],\\n       [-0.05632387,  0.07539415,  0.03932199, ..., -0.04766873,\\n         0.02875637,  0.06874508],\\n       [-0.03600975,  0.01135702, -0.07762122, ..., -0.01793904,\\n        -0.04088428,  0.06563687],\\n       ...,\\n       [-0.01409819, -0.01114139,  0.00948918, ..., -0.06594594,\\n         0.05415611, -0.05522961],\\n       [-0.00039982, -0.05005729, -0.04146854, ...,  0.07435285,\\n        -0.07084067,  0.03079193],\\n       [-0.06574259,  0.04187585, -0.04515645, ..., -0.06178195,\\n         0.02637964,  0.03797305]], dtype=float32)>, <tf.Tensor: shape=(200,), dtype=float32, numpy=\\narray([ 0.00357905, -0.00328432,  0.06080082,  0.01316852,  0.01048604,\\n       -0.00469753, -0.00777614,  0.03704596,  0.01874357,  0.02811878,\\n        0.0321062 ,  0.01702331,  0.02388108,  0.00559376,  0.05674131,\\n        0.01446631,  0.03809216,  0.00054441, -0.02106382,  0.01626423,\\n        0.01390268,  0.00171229,  0.02418429, -0.05647182,  0.03400285,\\n        0.00045802,  0.00237205,  0.08193516, -0.0051987 ,  0.03949347,\\n        0.03063275,  0.00242178,  0.02123609,  0.06985872, -0.00310036,\\n       -0.02126253,  0.0102579 ,  0.01942734,  0.0320902 , -0.02254228,\\n        0.0284302 ,  0.03331467,  0.02122299,  0.0160244 , -0.02989974,\\n       -0.01863906,  0.01002368, -0.03431645, -0.00312885, -0.00407348,\\n        0.0032425 ,  0.0052125 ,  0.03359714,  0.03633137,  0.01834046,\\n       -0.05762351,  0.03105312,  0.02606356,  0.03393162,  0.05006683,\\n       -0.02312271, -0.03820622,  0.00651098,  0.03870389,  0.00664978,\\n        0.02091726, -0.00780349,  0.01707618,  0.00681628,  0.05680835,\\n        0.0105384 ,  0.02772018,  0.01892942,  0.04603265,  0.07113411,\\n        0.05366849,  0.00230638,  0.02461458,  0.00439265,  0.0363445 ,\\n       -0.00203665,  0.05299354,  0.02521348,  0.06713161,  0.02187366,\\n        0.02421261, -0.05517467, -0.02240167,  0.0311252 ,  0.00343621,\\n       -0.01266517,  0.00164753,  0.0024896 ,  0.05805989,  0.03682691,\\n       -0.00359276,  0.02695322, -0.04413403,  0.02411772,  0.04473445,\\n        0.0038169 ,  0.02260166,  0.05256291,  0.01522107,  0.08006835,\\n        0.02948065,  0.01073819,  0.01792921,  0.01584445,  0.03479911,\\n        0.00293166, -0.03384708,  0.00868138,  0.00469696, -0.02755488,\\n        0.00204522,  0.07610419,  0.06294814, -0.03234263,  0.00433147,\\n        0.00025588,  0.00282683,  0.00504234,  0.00350097, -0.00903062,\\n        0.0104758 , -0.01934223, -0.0029659 , -0.00754172, -0.01211576,\\n       -0.05744886,  0.08374874,  0.0107411 , -0.01966254,  0.07429487,\\n        0.04483746, -0.00096984,  0.04564293, -0.02032583, -0.02058929,\\n        0.01182606,  0.00884634,  0.04335617,  0.01476672,  0.01212368,\\n        0.00848659, -0.00885709,  0.0070958 ,  0.01461094,  0.00579505,\\n       -0.03724125, -0.01389085, -0.04474322,  0.01357912,  0.00339645,\\n        0.00680559, -0.031376  ,  0.01389866, -0.02206327, -0.04290064,\\n       -0.02373166, -0.02124575,  0.06841339,  0.04832247,  0.03036575,\\n        0.0259193 , -0.00505115,  0.02555768, -0.02827518,  0.01202113,\\n        0.01990676, -0.05268514,  0.04168257, -0.01215763, -0.03520664,\\n        0.01080412,  0.00705321, -0.01901105, -0.0196851 ,  0.01229367,\\n        0.00938784,  0.01815698,  0.02437338, -0.0293498 ,  0.06587324,\\n        0.00998327,  0.03781283,  0.0181384 , -0.01826532,  0.0124462 ,\\n        0.05151324,  0.00669256,  0.01886022,  0.0041451 ,  0.0538783 ,\\n        0.062018  ,  0.04669508, -0.0299913 ,  0.00032858, -0.08542607],\\n      dtype=float32)>, <tf.Tensor: shape=(200, 200), dtype=float32, numpy=\\narray([[-0.0828221 , -0.14698574, -0.10051594, ..., -0.04628355,\\n        -0.1318373 , -0.09354172],\\n       [-0.02165342,  0.08431498, -0.08988205, ..., -0.12802598,\\n        -0.05818818,  0.08191217],\\n       [ 0.11813003,  0.12293847, -0.06109512, ...,  0.11996734,\\n         0.18356287,  0.08330926],\\n       ...,\\n       [-0.0508213 , -0.02975821, -0.01707401, ..., -0.13473095,\\n        -0.00646437,  0.16870573],\\n       [ 0.05848129,  0.0641664 , -0.05177091, ..., -0.08931366,\\n         0.07677906, -0.03542285],\\n       [ 0.09893051,  0.05218484, -0.13223168, ..., -0.03454452,\\n        -0.06776383,  0.10204276]], dtype=float32)>, <tf.Tensor: shape=(200,), dtype=float32, numpy=\\narray([-0.0087126 ,  0.02085234,  0.00299268,  0.05133496,  0.07153282,\\n       -0.00719226, -0.00331099,  0.01199473,  0.0192128 ,  0.0085464 ,\\n       -0.01787003,  0.02216839,  0.02569606,  0.02581725,  0.01841954,\\n        0.01116915, -0.00086344,  0.00689635,  0.02229806, -0.00417295,\\n       -0.00686153,  0.00129532,  0.02566698, -0.00501552, -0.01111127,\\n        0.0479724 ,  0.03123751,  0.04617535,  0.03156878,  0.03900028,\\n        0.05560375, -0.0098789 ,  0.00250087, -0.02011788, -0.00230746,\\n        0.03645366,  0.01779543,  0.00702731, -0.02867669,  0.02719683,\\n        0.02676964,  0.00146008, -0.05534646,  0.00197113,  0.00777692,\\n       -0.04033504,  0.03605822, -0.05283628,  0.01147152,  0.00828043,\\n        0.03759411, -0.0100305 ,  0.02789086,  0.02655328, -0.00973935,\\n        0.05213516,  0.02224953,  0.00722343, -0.00033468, -0.00711926,\\n        0.00985263, -0.01073282, -0.00344675,  0.06129129,  0.04092487,\\n        0.00694012, -0.01582139,  0.03066047, -0.01300736,  0.0080664 ,\\n        0.00691204,  0.02191243, -0.0066635 , -0.01320587,  0.00172126,\\n        0.02277966,  0.0047523 ,  0.00051841,  0.01545054,  0.02186972,\\n        0.00760062,  0.03432304,  0.00082327,  0.01123053,  0.01636643,\\n        0.02763152, -0.01168553,  0.04091002, -0.00228446,  0.0250755 ,\\n        0.01590482, -0.00991806, -0.00616865,  0.00235936, -0.00094153,\\n        0.02754891,  0.01968959,  0.00238424,  0.0157536 ,  0.00684924,\\n        0.05181101, -0.02254258,  0.00705285, -0.00126077, -0.00554936,\\n        0.02230443, -0.00128832,  0.00501818,  0.10632959, -0.02039386,\\n        0.00940753, -0.0020189 , -0.00541742,  0.0076806 ,  0.00788476,\\n        0.00642538,  0.00631363,  0.0114892 ,  0.01805533, -0.00032522,\\n       -0.02330949,  0.00827862,  0.03791269,  0.08430569,  0.03085025,\\n        0.02776445,  0.02011739, -0.02219705,  0.06465723,  0.04204341,\\n        0.00106569,  0.00863791,  0.01861478, -0.00086186,  0.04127715,\\n        0.04919536, -0.00607483,  0.01782548,  0.01881419,  0.02882998,\\n       -0.04134833, -0.01938913,  0.04004768,  0.03608599,  0.02072833,\\n        0.01351104,  0.02837778, -0.00048319,  0.06932736, -0.00106327,\\n        0.01861584,  0.00429863,  0.00679461,  0.02634298,  0.01668091,\\n       -0.03211238,  0.02500285,  0.00461276, -0.02165896,  0.05607646,\\n       -0.00127778, -0.00471196,  0.00995572, -0.01569978,  0.07521033,\\n        0.02028742,  0.01239037,  0.00529423,  0.01372857,  0.04420771,\\n        0.03237991, -0.00445859,  0.01318046,  0.02035166, -0.00367758,\\n        0.0314094 ,  0.00066138,  0.00157378, -0.01251531,  0.06994109,\\n        0.00034664,  0.00264126,  0.01538078,  0.00017881,  0.0030126 ,\\n        0.05441034, -0.00207278,  0.0171648 ,  0.00382356,  0.00101738,\\n        0.02338391,  0.02909268,  0.00042468,  0.04246379, -0.01114535,\\n        0.01220505,  0.03153162,  0.00342415,  0.0555251 ,  0.03160082],\\n      dtype=float32)>, <tf.Tensor: shape=(200, 10), dtype=float32, numpy=\\narray([[-0.21038382, -0.03303704,  0.07651581, ...,  0.06928309,\\n        -0.21602298,  0.01948527],\\n       [-0.02638181,  0.09460881, -0.22671933, ...,  0.2716653 ,\\n        -0.05000273,  0.02951122],\\n       [-0.11970981, -0.05185533,  0.15795602, ...,  0.01756519,\\n        -0.1341304 , -0.03688926],\\n       ...,\\n       [-0.16229622,  0.0775655 ,  0.14931446, ...,  0.12711264,\\n        -0.1364896 , -0.10984147],\\n       [-0.1013392 ,  0.05423185, -0.24538445, ...,  0.20693792,\\n        -0.13455108, -0.14477421],\\n       [-0.07005077, -0.22477177, -0.24911332, ..., -0.28207096,\\n         0.03002683,  0.01892361]], dtype=float32)>, <tf.Tensor: shape=(10,), dtype=float32, numpy=\\narray([-0.1340848 ,  0.12615652, -0.05336644, -0.05045449,  0.04629441,\\n        0.18149577, -0.02235509, -0.00183215, -0.09643064,  0.00457702],\\n      dtype=float32)>]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(average_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export architecture to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_string = global_model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 784], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"batch_input_shape\": [null, 784], \"units\": 200, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation\", \"trainable\": true, \"dtype\": \"float32\", \"activation\": \"relu\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 200, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation_1\", \"trainable\": true, \"dtype\": \"float32\", \"activation\": \"relu\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 10, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation_2\", \"trainable\": true, \"dtype\": \"float32\", \"activation\": \"softmax\"}}]}, \"keras_version\": \"2.11.0\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export weights as h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.save_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('weights.h5', 'r')\n",
    "# print(list(f.keys()))\n",
    "# # will get a list of layer names which you can use as index\n",
    "# d = f['dense']['dense_1']['kernel:0']\n",
    "# # <HDF5 dataset \"kernel:0\": shape (128, 1), type \"<f4\">\n",
    "# d.shape == (128, 1)\n",
    "# d[0] == array([-0.14390108], dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"weights.h5\" (mode r)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export weights as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(average_weights, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Global model aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "# update global model\n",
    "global_model.set_weights(average_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b07f451ebbbe1f80453345214d70880b5111d33133c65ac28c98b3e601695cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
