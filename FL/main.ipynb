{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nextg3/miniconda3/envs/fl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-02-06 12:00:55.557416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-06 12:00:55.802308: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-06 12:00:55.839482: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nextg3/miniconda3/envs/fl/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-06 12:00:55.839503: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-06 12:00:56.780865: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nextg3/miniconda3/envs/fl/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-06 12:00:56.780925: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nextg3/miniconda3/envs/fl/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-06 12:00:56.780930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D \n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "from tensorflow.keras import backend as k\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(paths, verbose=-1):\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    # Loop over input images\n",
    "    for (i,imgpath) in enumerate(paths):\n",
    "        #  load image and extract label\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.array(im_gray).flatten()\n",
    "        label = imgpath.split(os.path.sep)[-2]\n",
    "        #  scale image\n",
    "        data.append(image/255)\n",
    "        labels.append(label)\n",
    "        #  show update\n",
    "        if verbose > 0 and i > 0 and (i+1) % verbose ==  0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
    "        #  return tuple mof data and label\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/60000\n",
      "[INFO] processed 20000/60000\n",
      "[INFO] processed 30000/60000\n",
      "[INFO] processed 40000/60000\n",
      "[INFO] processed 50000/60000\n",
      "[INFO] processed 60000/60000\n"
     ]
    }
   ],
   "source": [
    "#  declare path\n",
    "# img_path = '/home/nextg3/Documents/FederatedLearning/FL_final/MNIST_training'\n",
    "img_path = '/home/nextg3/Documents/Thesis/Code/FL/MNIST_training'\n",
    "# Get path list\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "#  apply function\n",
    "image_list, label_list = load(image_paths, verbose = 10000)\n",
    "#  Binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n",
    "# split train test\n",
    "X_train,X_test, y_train, y_test = train_test_split(image_list,\n",
    "                                                   label_list,\n",
    "                                                   test_size=0.1,\n",
    "                                                   random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
    "    '''returns a dictionary with keys as client names and values as data shards - (image, label)\n",
    "        args:\n",
    "    \n",
    "                '''\n",
    "    #  create list of client names\n",
    "    client_names = ['{}_{}'.format(initial,i) for i in range(num_clients)]\n",
    "    #  randomize data\n",
    "    data = list(zip(image_list,label_list))\n",
    "    random.shuffle(data)\n",
    "    #  place data at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i+size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    # no of shrads should equa no of clients\n",
    "    assert(len(shards)==len(client_names))\n",
    "\n",
    "    return {client_names[i]:shards[i] for i in range(len(client_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create clients\n",
    "clients = create_clients(X_train,y_train,num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''client data shard -> tfds object\n",
    "        args:\n",
    "            shard\n",
    "            batch size\n",
    "        return\n",
    "            tfds object'''\n",
    "    #  sep data and label\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data),list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 12:01:05.149234: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nextg3/miniconda3/envs/fl/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-06 12:01:05.149468: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-06 12:01:05.149485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DELL-XPS-13-9305): /proc/driver/nvidia/version does not exist\n",
      "2023-02-06 12:01:05.151128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Process and batch data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "#  Processed and batch the test set\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "coms_round = 100\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(learning_rate = lr,\n",
    "                momentum = 0.9\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scaling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    # get bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    # print('bs',bs)\n",
    "    # Calculater the total data points across the clients\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # print('glocal_count: ',global_count)\n",
    "    # get total no of data points held by the client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    # print('local_count: ',local_count)\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''Function for scaling a model's weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar*weight[i])\n",
    "    return weight_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  sum_scaled_weights(scaled_weight_list):\n",
    "    '''Returns the sum of the listed scaled weights.\n",
    "    scaled average of the weights'''\n",
    "    avg_grad = list()\n",
    "    #  get the average grad over all client gradients\n",
    "    for grad_list_tupel in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tupel, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test, model, coms_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    # logits=model.predict(X_test, batch_size=100)\n",
    "    logits=model.predict(X_test)\n",
    "    loss = cce(y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis  = 1), tf.argmax(Y_test, axis = 1))\n",
    "    print('coom_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 630us/step\n",
      "coom_round: 0 | global_acc: 90.283% | global_loss: 1.6277220249176025\n",
      "188/188 [==============================] - 0s 619us/step\n",
      "coom_round: 1 | global_acc: 92.183% | global_loss: 1.5928096771240234\n",
      "188/188 [==============================] - 0s 608us/step\n",
      "coom_round: 2 | global_acc: 93.150% | global_loss: 1.5721532106399536\n",
      "188/188 [==============================] - 0s 612us/step\n",
      "coom_round: 3 | global_acc: 94.067% | global_loss: 1.5593196153640747\n",
      "188/188 [==============================] - 0s 748us/step\n",
      "coom_round: 4 | global_acc: 94.783% | global_loss: 1.5484628677368164\n",
      "188/188 [==============================] - 0s 614us/step\n",
      "coom_round: 5 | global_acc: 95.167% | global_loss: 1.5394554138183594\n",
      "188/188 [==============================] - 0s 629us/step\n",
      "coom_round: 6 | global_acc: 95.583% | global_loss: 1.5330924987792969\n",
      "188/188 [==============================] - 0s 625us/step\n",
      "coom_round: 7 | global_acc: 95.767% | global_loss: 1.528336763381958\n",
      "188/188 [==============================] - 0s 618us/step\n",
      "coom_round: 8 | global_acc: 96.050% | global_loss: 1.5237685441970825\n",
      "188/188 [==============================] - 0s 619us/step\n",
      "coom_round: 9 | global_acc: 96.117% | global_loss: 1.5193227529525757\n"
     ]
    }
   ],
   "source": [
    "# Initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784,10)\n",
    "\n",
    "comms_round = 10\n",
    "\n",
    "# start global training loop\n",
    "for comm_round in range(comms_round):\n",
    "    #  get the global model's weights - initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "\n",
    "    # List to collect local model weights after scaling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    # Randomize client data using keys\n",
    "    client_names = list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "\n",
    "    # Loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784,10)\n",
    "        local_model.compile(loss=loss,\n",
    "                            optimizer=optimizer,\n",
    "                            metrics=metrics)\n",
    "        # set weights of the global model as weights of the local modelk\n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        # Fit local model with client data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "\n",
    "        # scale the model weights and add to the list\n",
    "        # scaling_factor = weight_scaling_factor(clients_batched, client)\n",
    "        # scaled_weights = scale_model_weights(local_model.get_weights(),scaling_factor)\n",
    "        # scaled_local_weightsmlp_local = SimpleMLP()\n",
    "        # local_model = smlp_local.build(784,10)\n",
    "        # local_model.compile(loss=loss,\n",
    "        #                     optimizer=optimizer,\n",
    "        #                     metrics=metrics)\n",
    "        # # set weights of the global model as weights of the local modelk\n",
    "        # local_model.set_weights(global_weights)\n",
    "\n",
    "        # # Fit local model with client data\n",
    "        # local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "\n",
    "        # scale the model weights and add to the list\n",
    "        scaling_factor = weight_scaling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(),scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "        # Clear session to clean memory\n",
    "        k.clear_session()\n",
    "    \n",
    "    # Get the average overall model\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "    # update global model\n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    # Test globalmodel\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[<tf.Tensor: shape=(784, 200), dtype=float32, numpy=\\narray([[-0.01040198, -0.05578183, -0.05357958, ..., -0.06769065,\\n        -0.04030259,  0.04086372],\\n       [ 0.06268313, -0.03997761, -0.07568463, ...,  0.00753239,\\n         0.02759007,  0.04249723],\\n       [-0.00582946,  0.0174942 , -0.02019585, ..., -0.01053266,\\n         0.02545312,  0.05473597],\\n       ...,\\n       [-0.01797263,  0.04460895,  0.02997313, ..., -0.0529381 ,\\n         0.0279701 , -0.06740224],\\n       [ 0.0193205 , -0.01978557, -0.0189405 , ..., -0.00771479,\\n        -0.02440459, -0.00386408],\\n       [ 0.06105473,  0.06315143,  0.01426234, ...,  0.03281127,\\n         0.04488137, -0.06221747]], dtype=float32)>, <tf.Tensor: shape=(200,), dtype=float32, numpy=\\narray([ 0.02121967,  0.01608862,  0.00875596,  0.01596739,  0.00928205,\\n       -0.01048895,  0.0279254 , -0.02941848, -0.02364868,  0.00264643,\\n       -0.01464437,  0.00828531,  0.00178428,  0.00848354,  0.03125082,\\n        0.10475545,  0.094607  ,  0.01048617,  0.01994915, -0.03153332,\\n        0.02420933, -0.05390339, -0.02373891,  0.00732616,  0.00764051,\\n       -0.01045249,  0.08234395, -0.01842859,  0.03569286,  0.01282165,\\n       -0.0096709 ,  0.00236068,  0.04111158,  0.02418845, -0.03393902,\\n       -0.03206053, -0.01576213,  0.04186013, -0.01312074,  0.01837479,\\n       -0.00655377,  0.03831899,  0.00060861,  0.0667126 , -0.0259556 ,\\n       -0.01254431,  0.03949718, -0.02381864,  0.00753092,  0.01843983,\\n        0.0296315 , -0.04900167,  0.06344075, -0.02339866,  0.01264788,\\n       -0.00554795,  0.02132797,  0.00162527, -0.0115077 , -0.01539956,\\n       -0.00273218,  0.04359703,  0.00684845, -0.02481978,  0.00637913,\\n        0.00104448, -0.02629861,  0.02901705, -0.00284779,  0.00824822,\\n       -0.00103288,  0.01700193,  0.03198385,  0.0099097 ,  0.01149406,\\n       -0.01200775, -0.02001258,  0.01554929,  0.01447998,  0.00181749,\\n        0.01868295, -0.0219407 ,  0.08939663,  0.03796397,  0.01499999,\\n        0.0421811 ,  0.0059088 , -0.05326711,  0.04035221,  0.01015802,\\n       -0.0056871 ,  0.0420756 ,  0.02351819,  0.02980997,  0.024488  ,\\n       -0.04240851, -0.02760554, -0.00855968,  0.0488552 , -0.02952208,\\n        0.07135662, -0.01792463,  0.02408249, -0.04104339,  0.03800205,\\n        0.01338169,  0.04455887,  0.0152869 , -0.03687671,  0.03546429,\\n        0.00822015, -0.00252314, -0.00896304,  0.01310033,  0.02543815,\\n        0.03263381, -0.00967529,  0.05599536, -0.00343916,  0.05481152,\\n        0.04341733,  0.0145157 ,  0.0155685 , -0.00742731,  0.04075094,\\n        0.00081669, -0.03423798,  0.04502873, -0.02421729, -0.02694387,\\n        0.04104796,  0.00379504, -0.0024929 ,  0.02691437, -0.00656883,\\n       -0.03051934,  0.03836281,  0.02699023, -0.0184167 ,  0.01726394,\\n        0.01925721, -0.01605069,  0.00158486, -0.00418697,  0.01954424,\\n        0.04063416,  0.00915   , -0.0062493 ,  0.01915407,  0.01110737,\\n       -0.052372  ,  0.00025242, -0.06362902,  0.1057148 , -0.00359229,\\n        0.02048393,  0.02527046,  0.0255424 ,  0.03342497, -0.00376374,\\n        0.07121634, -0.00692713,  0.04265197, -0.04073206,  0.0185965 ,\\n        0.03481731,  0.02753979,  0.0371094 , -0.03495975,  0.0200328 ,\\n        0.04122156,  0.00099735,  0.02511195,  0.00226866,  0.07124112,\\n        0.04481784,  0.0087336 , -0.0031573 , -0.00661463,  0.07992976,\\n        0.02885569, -0.01563139, -0.03595588,  0.07412517, -0.05546038,\\n       -0.00483233,  0.08975475,  0.00610161, -0.00603608,  0.0179806 ,\\n        0.00744163,  0.0818376 ,  0.00939682,  0.00518019,  0.0191279 ,\\n       -0.01123546,  0.00466535,  0.00504401,  0.01694992,  0.02447925],\\n      dtype=float32)>, <tf.Tensor: shape=(200, 200), dtype=float32, numpy=\\narray([[ 0.03564722, -0.05699871,  0.00235253, ...,  0.04511095,\\n        -0.12460632,  0.05926352],\\n       [ 0.08934097, -0.02750622,  0.0624579 , ..., -0.07706563,\\n         0.02114522, -0.05457603],\\n       [ 0.0331906 , -0.09460098,  0.08056506, ...,  0.06883486,\\n        -0.00519435,  0.1380069 ],\\n       ...,\\n       [-0.05333879,  0.06454512, -0.01022818, ...,  0.05425516,\\n        -0.03721665,  0.09204648],\\n       [-0.05261209,  0.01550849,  0.02985891, ...,  0.00709138,\\n         0.02133115, -0.03135247],\\n       [-0.05714167,  0.10536047,  0.12541643, ...,  0.01200993,\\n         0.00193647, -0.05775852]], dtype=float32)>, <tf.Tensor: shape=(200,), dtype=float32, numpy=\\narray([ 0.0080943 ,  0.01556311, -0.01290536,  0.05845323,  0.03577554,\\n       -0.0167608 ,  0.06223218,  0.03836257,  0.04146844,  0.02685494,\\n        0.0035852 ,  0.00428461,  0.06175804, -0.00939916, -0.00636715,\\n        0.01233121,  0.05833146, -0.02383755, -0.06869186, -0.00016454,\\n       -0.00044236,  0.03570272,  0.01647443,  0.07050246,  0.02732953,\\n        0.01330456,  0.04057515,  0.00033461,  0.03225518,  0.01012018,\\n        0.0517761 ,  0.03221389,  0.0130215 ,  0.02096572,  0.02841329,\\n        0.01929347,  0.01662274, -0.01736803, -0.0033321 ,  0.01779746,\\n       -0.02194242,  0.02216316,  0.03272079, -0.00072558,  0.03571863,\\n       -0.00496903, -0.03205548, -0.00875737, -0.01955467, -0.01429642,\\n        0.01004341, -0.01074087,  0.03454517,  0.05569107,  0.0132845 ,\\n       -0.0398658 , -0.01306907,  0.0947693 , -0.02647688,  0.0037699 ,\\n       -0.02924133,  0.02363832, -0.04596979, -0.00416803, -0.0023847 ,\\n       -0.00093905,  0.02109659,  0.02783381,  0.01833169,  0.05046017,\\n        0.04180739,  0.02044056,  0.00713543, -0.02325787, -0.00173342,\\n       -0.00072051,  0.00475136,  0.00763319,  0.06317759,  0.00670503,\\n       -0.02798733,  0.03587075,  0.04133666, -0.00607785,  0.00344717,\\n       -0.01007816,  0.00846651, -0.00550055,  0.02956381,  0.02617502,\\n        0.02788247,  0.04184937,  0.03397767,  0.02407506,  0.01875041,\\n        0.00951034, -0.02737963,  0.02907035,  0.00983754, -0.00519567,\\n        0.03800474, -0.00161526,  0.0684297 ,  0.06401165,  0.01193717,\\n        0.01475393,  0.03047343,  0.01177612, -0.01292536,  0.00419549,\\n        0.02087588,  0.00343049, -0.01227595,  0.02335862, -0.01363231,\\n       -0.01947434, -0.0110532 ,  0.05401687,  0.02271104, -0.02226343,\\n        0.03925382,  0.00090315, -0.00029702, -0.01216922, -0.01936154,\\n        0.02843221, -0.01655154,  0.02379211,  0.03158744,  0.01553136,\\n        0.06998956, -0.0072753 ,  0.02604971,  0.06129504,  0.05722603,\\n        0.00102214,  0.01608448,  0.00649976, -0.01408139, -0.01633841,\\n        0.04432535,  0.0353222 , -0.01200916,  0.0067941 ,  0.01058491,\\n        0.01118667,  0.05536862, -0.00858313,  0.01036527,  0.03875085,\\n        0.05489456,  0.01894286,  0.03328117,  0.00159715,  0.01393664,\\n        0.02675421,  0.00268322, -0.03238812,  0.04455951, -0.00573746,\\n        0.05605559,  0.00302267, -0.01005055,  0.02832526,  0.03813112,\\n       -0.02971174,  0.02958149,  0.02584693,  0.02758537, -0.00194529,\\n        0.00874521, -0.00078988,  0.01457764,  0.01814766, -0.05464334,\\n        0.03301127,  0.00216171, -0.00177928,  0.01633849, -0.01182738,\\n        0.04510687,  0.07556812,  0.0623058 ,  0.00093827,  0.0190283 ,\\n        0.03038952, -0.0259656 ,  0.00052053,  0.04245138,  0.07217486,\\n        0.04994623,  0.01476236, -0.0187513 ,  0.02175376,  0.02166675,\\n       -0.00914282, -0.00054044, -0.02589477,  0.01262903, -0.01392336],\\n      dtype=float32)>, <tf.Tensor: shape=(200, 10), dtype=float32, numpy=\\narray([[-0.05794006,  0.05789828,  0.13437168, ..., -0.03204243,\\n         0.04050279,  0.04224657],\\n       [ 0.18905927,  0.0142795 ,  0.00170781, ...,  0.03307711,\\n         0.03185114, -0.1868657 ],\\n       [-0.04341641, -0.02194378, -0.14796   , ...,  0.05828487,\\n         0.0171553 , -0.12771797],\\n       ...,\\n       [ 0.31607038, -0.17628661,  0.16461058, ..., -0.22448921,\\n         0.18437809, -0.17311247],\\n       [ 0.09349901, -0.03123641, -0.08767647, ...,  0.06112146,\\n        -0.1715183 ,  0.16119742],\\n       [ 0.03387531,  0.03579252, -0.0541546 , ...,  0.30593303,\\n        -0.29165277, -0.16677442]], dtype=float32)>, <tf.Tensor: shape=(10,), dtype=float32, numpy=\\narray([-0.09807862,  0.10143049, -0.05115442, -0.05519567,  0.05343405,\\n        0.12744917, -0.08020999,  0.06016795, -0.10232425,  0.0444813 ],\\n      dtype=float32)>]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(average_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export architecture to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_string = global_model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 784], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"batch_input_shape\": [null, 784], \"units\": 200, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation\", \"trainable\": true, \"dtype\": \"float32\", \"activation\": \"relu\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 200, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation_1\", \"trainable\": true, \"dtype\": \"float32\", \"activation\": \"relu\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 10, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation_2\", \"trainable\": true, \"dtype\": \"float32\", \"activation\": \"softmax\"}}]}, \"keras_version\": \"2.11.0\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export weights as h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.save_weights('weights.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export weights as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(average_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b07f451ebbbe1f80453345214d70880b5111d33133c65ac28c98b3e601695cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
