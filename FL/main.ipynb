{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D \n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "from tensorflow.keras import backend as k\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(paths, verbose=-1):\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    # Loop over input images\n",
    "    for (i,imgpath) in enumerate(paths):\n",
    "        #  load image and extract label\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.array(im_gray).flatten()\n",
    "        label = imgpath.split(os.path.sep)[-2]\n",
    "        #  scale image\n",
    "        data.append(image/255)\n",
    "        labels.append(label)\n",
    "        #  show update\n",
    "        if verbose > 0 and i > 0 and (i+1) % verbose ==  0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
    "        #  return tuple mof data and label\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/60000\n",
      "[INFO] processed 20000/60000\n",
      "[INFO] processed 30000/60000\n",
      "[INFO] processed 40000/60000\n",
      "[INFO] processed 50000/60000\n",
      "[INFO] processed 60000/60000\n"
     ]
    }
   ],
   "source": [
    "#  declare path\n",
    "img_path = '/home/nextg3/Documents/FederatedLearning/FL_final/MNIST_training'\n",
    "# Get path list\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "#  apply function\n",
    "image_list, label_list = load(image_paths, verbose = 10000)\n",
    "#  Binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n",
    "# split train test\n",
    "X_train,X_test, y_train, y_test = train_test_split(image_list,\n",
    "                                                   label_list,\n",
    "                                                   test_size=0.1,\n",
    "                                                   random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
    "    '''returns a dictionary with keys as client names and values as data shards - (image, label)\n",
    "        args:\n",
    "    \n",
    "                '''\n",
    "    #  create list of client names\n",
    "    client_names = ['{}_{}'.format(initial,i) for i in range(num_clients)]\n",
    "    #  randomize data\n",
    "    data = list(zip(image_list,label_list))\n",
    "    random.shuffle(data)\n",
    "    #  place data at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i+size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    # no of shrads should equa no of clients\n",
    "    assert(len(shards)==len(client_names))\n",
    "\n",
    "    return {client_names[i]:shards[i] for i in range(len(client_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create clients\n",
    "clients = create_clients(X_train,y_train,num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''client data shard -> tfds object\n",
    "        args:\n",
    "            shard\n",
    "            batch size\n",
    "        return\n",
    "            tfds object'''\n",
    "    #  sep data and label\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data),list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and batch data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "#  Processed and batch the test set\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "coms_round = 100\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(learning_rate = lr,\n",
    "                momentum = 0.9\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scaling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    # get bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    # Calculater the total data points across the clients\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get total no of data points held by the client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''Function for scaling a model's weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar*weight[i])\n",
    "    return weight_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  sum_scaled_weights(scaled_weight_list):\n",
    "    '''Returns the sum of the listed scaled weights.\n",
    "    scaled average of the weights'''\n",
    "    avg_grad = list()\n",
    "    #  get the average grad over all client gradients\n",
    "    for grad_list_tupel in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tupel, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test, model, coms_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    # logits=model.predict(X_test, batch_size=100)\n",
    "    logits=model.predict(X_test)\n",
    "    loss = cce(y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis  = 1), tf.argmax(Y_test, axis = 1))\n",
    "    print('coom_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 625us/step\n",
      "coom_round: 0 | global_acc: 90.000% | global_loss: 1.6354068517684937\n",
      "188/188 [==============================] - 0s 628us/step\n",
      "coom_round: 1 | global_acc: 92.217% | global_loss: 1.5910351276397705\n",
      "188/188 [==============================] - 0s 609us/step\n",
      "coom_round: 2 | global_acc: 93.317% | global_loss: 1.5720058679580688\n",
      "188/188 [==============================] - 0s 627us/step\n",
      "coom_round: 3 | global_acc: 94.233% | global_loss: 1.5576733350753784\n",
      "188/188 [==============================] - 0s 607us/step\n",
      "coom_round: 4 | global_acc: 94.733% | global_loss: 1.5493097305297852\n",
      "188/188 [==============================] - 0s 624us/step\n",
      "coom_round: 5 | global_acc: 95.183% | global_loss: 1.5399550199508667\n",
      "188/188 [==============================] - 0s 617us/step\n",
      "coom_round: 6 | global_acc: 95.367% | global_loss: 1.5345598459243774\n",
      "188/188 [==============================] - 0s 663us/step\n",
      "coom_round: 7 | global_acc: 95.767% | global_loss: 1.5275026559829712\n",
      "188/188 [==============================] - 0s 620us/step\n",
      "coom_round: 8 | global_acc: 96.050% | global_loss: 1.5242358446121216\n",
      "188/188 [==============================] - 0s 611us/step\n",
      "coom_round: 9 | global_acc: 96.033% | global_loss: 1.5208752155303955\n",
      "188/188 [==============================] - 0s 660us/step\n",
      "coom_round: 10 | global_acc: 96.400% | global_loss: 1.5171082019805908\n",
      "188/188 [==============================] - 0s 651us/step\n",
      "coom_round: 11 | global_acc: 96.567% | global_loss: 1.5144171714782715\n",
      "188/188 [==============================] - 0s 647us/step\n",
      "coom_round: 12 | global_acc: 96.733% | global_loss: 1.5113435983657837\n",
      "188/188 [==============================] - 0s 624us/step\n",
      "coom_round: 13 | global_acc: 96.933% | global_loss: 1.5095585584640503\n",
      "188/188 [==============================] - 0s 615us/step\n",
      "coom_round: 14 | global_acc: 97.017% | global_loss: 1.5079327821731567\n",
      "188/188 [==============================] - 0s 615us/step\n",
      "coom_round: 15 | global_acc: 97.167% | global_loss: 1.5061392784118652\n",
      "188/188 [==============================] - 0s 620us/step\n",
      "coom_round: 16 | global_acc: 97.333% | global_loss: 1.5039441585540771\n",
      "188/188 [==============================] - 0s 624us/step\n",
      "coom_round: 17 | global_acc: 97.333% | global_loss: 1.5032392740249634\n",
      "188/188 [==============================] - 0s 630us/step\n",
      "coom_round: 18 | global_acc: 97.517% | global_loss: 1.5011084079742432\n",
      "188/188 [==============================] - 0s 624us/step\n",
      "coom_round: 19 | global_acc: 97.533% | global_loss: 1.4998196363449097\n",
      "188/188 [==============================] - 0s 666us/step\n",
      "coom_round: 20 | global_acc: 97.567% | global_loss: 1.4987454414367676\n",
      "188/188 [==============================] - 0s 622us/step\n",
      "coom_round: 21 | global_acc: 97.733% | global_loss: 1.4975121021270752\n",
      "188/188 [==============================] - 0s 611us/step\n",
      "coom_round: 22 | global_acc: 97.600% | global_loss: 1.497132658958435\n",
      "188/188 [==============================] - 0s 618us/step\n",
      "coom_round: 23 | global_acc: 97.750% | global_loss: 1.496391773223877\n",
      "188/188 [==============================] - 0s 617us/step\n",
      "coom_round: 24 | global_acc: 97.683% | global_loss: 1.4955475330352783\n",
      "188/188 [==============================] - 0s 617us/step\n",
      "coom_round: 25 | global_acc: 97.667% | global_loss: 1.494686484336853\n",
      "188/188 [==============================] - 0s 649us/step\n",
      "coom_round: 26 | global_acc: 97.750% | global_loss: 1.4940433502197266\n",
      "188/188 [==============================] - 0s 679us/step\n",
      "coom_round: 27 | global_acc: 97.867% | global_loss: 1.4933961629867554\n",
      "188/188 [==============================] - 0s 657us/step\n",
      "coom_round: 28 | global_acc: 97.850% | global_loss: 1.4931609630584717\n",
      "188/188 [==============================] - 0s 609us/step\n",
      "coom_round: 29 | global_acc: 97.950% | global_loss: 1.4921237230300903\n",
      "188/188 [==============================] - 0s 610us/step\n",
      "coom_round: 30 | global_acc: 97.817% | global_loss: 1.491405963897705\n",
      "188/188 [==============================] - 0s 603us/step\n",
      "coom_round: 31 | global_acc: 97.800% | global_loss: 1.4918122291564941\n",
      "188/188 [==============================] - 0s 624us/step\n",
      "coom_round: 32 | global_acc: 97.933% | global_loss: 1.4905699491500854\n",
      "188/188 [==============================] - 0s 612us/step\n",
      "coom_round: 33 | global_acc: 97.817% | global_loss: 1.49089777469635\n",
      "188/188 [==============================] - 0s 603us/step\n",
      "coom_round: 34 | global_acc: 97.850% | global_loss: 1.490171194076538\n",
      "188/188 [==============================] - 0s 606us/step\n",
      "coom_round: 35 | global_acc: 98.017% | global_loss: 1.489348292350769\n",
      "188/188 [==============================] - 0s 661us/step\n",
      "coom_round: 36 | global_acc: 98.117% | global_loss: 1.4886924028396606\n",
      "188/188 [==============================] - 0s 632us/step\n",
      "coom_round: 37 | global_acc: 97.933% | global_loss: 1.4891077280044556\n",
      "188/188 [==============================] - 0s 619us/step\n",
      "coom_round: 38 | global_acc: 98.117% | global_loss: 1.4879730939865112\n",
      "188/188 [==============================] - 0s 654us/step\n",
      "coom_round: 39 | global_acc: 98.033% | global_loss: 1.488157868385315\n",
      "188/188 [==============================] - 0s 629us/step\n",
      "coom_round: 40 | global_acc: 97.983% | global_loss: 1.4878066778182983\n",
      "188/188 [==============================] - 0s 627us/step\n",
      "coom_round: 41 | global_acc: 98.050% | global_loss: 1.48749840259552\n",
      "188/188 [==============================] - 0s 616us/step\n",
      "coom_round: 42 | global_acc: 97.983% | global_loss: 1.4872663021087646\n",
      "188/188 [==============================] - 0s 671us/step\n",
      "coom_round: 43 | global_acc: 98.017% | global_loss: 1.4874788522720337\n",
      "188/188 [==============================] - 0s 625us/step\n",
      "coom_round: 44 | global_acc: 98.117% | global_loss: 1.4868979454040527\n",
      "188/188 [==============================] - 0s 610us/step\n",
      "coom_round: 45 | global_acc: 98.150% | global_loss: 1.4865303039550781\n",
      "188/188 [==============================] - 0s 630us/step\n",
      "coom_round: 46 | global_acc: 98.033% | global_loss: 1.4867562055587769\n",
      "188/188 [==============================] - 0s 616us/step\n",
      "coom_round: 47 | global_acc: 98.033% | global_loss: 1.4867470264434814\n",
      "188/188 [==============================] - 0s 604us/step\n",
      "coom_round: 48 | global_acc: 98.117% | global_loss: 1.485988974571228\n",
      "188/188 [==============================] - 0s 626us/step\n",
      "coom_round: 49 | global_acc: 98.167% | global_loss: 1.4854633808135986\n",
      "188/188 [==============================] - 0s 608us/step\n",
      "coom_round: 50 | global_acc: 98.100% | global_loss: 1.4856815338134766\n",
      "188/188 [==============================] - 0s 621us/step\n",
      "coom_round: 51 | global_acc: 98.050% | global_loss: 1.4855848550796509\n",
      "188/188 [==============================] - 0s 623us/step\n",
      "coom_round: 52 | global_acc: 98.117% | global_loss: 1.4854873418807983\n",
      "188/188 [==============================] - 0s 608us/step\n",
      "coom_round: 53 | global_acc: 98.150% | global_loss: 1.485270380973816\n",
      "188/188 [==============================] - 0s 618us/step\n",
      "coom_round: 54 | global_acc: 98.133% | global_loss: 1.4851213693618774\n",
      "188/188 [==============================] - 0s 612us/step\n",
      "coom_round: 55 | global_acc: 98.167% | global_loss: 1.484973430633545\n",
      "188/188 [==============================] - 0s 608us/step\n",
      "coom_round: 56 | global_acc: 98.150% | global_loss: 1.4849700927734375\n",
      "188/188 [==============================] - 0s 611us/step\n",
      "coom_round: 57 | global_acc: 98.083% | global_loss: 1.485343098640442\n",
      "188/188 [==============================] - 0s 700us/step\n",
      "coom_round: 58 | global_acc: 98.150% | global_loss: 1.4845428466796875\n",
      "188/188 [==============================] - 0s 605us/step\n",
      "coom_round: 59 | global_acc: 98.133% | global_loss: 1.4846022129058838\n",
      "188/188 [==============================] - 0s 611us/step\n",
      "coom_round: 60 | global_acc: 98.067% | global_loss: 1.4850393533706665\n",
      "188/188 [==============================] - 0s 644us/step\n",
      "coom_round: 61 | global_acc: 98.150% | global_loss: 1.4845328330993652\n",
      "188/188 [==============================] - 0s 612us/step\n",
      "coom_round: 62 | global_acc: 98.150% | global_loss: 1.4843732118606567\n",
      "188/188 [==============================] - 0s 605us/step\n",
      "coom_round: 63 | global_acc: 98.117% | global_loss: 1.4841357469558716\n",
      "188/188 [==============================] - 0s 661us/step\n",
      "coom_round: 64 | global_acc: 98.100% | global_loss: 1.4842725992202759\n",
      "188/188 [==============================] - 0s 608us/step\n",
      "coom_round: 65 | global_acc: 98.217% | global_loss: 1.4838612079620361\n",
      "188/188 [==============================] - 0s 609us/step\n",
      "coom_round: 66 | global_acc: 98.083% | global_loss: 1.4840644598007202\n",
      "188/188 [==============================] - 0s 614us/step\n",
      "coom_round: 67 | global_acc: 98.167% | global_loss: 1.4837615489959717\n",
      "188/188 [==============================] - 0s 670us/step\n",
      "coom_round: 68 | global_acc: 98.200% | global_loss: 1.483892560005188\n",
      "188/188 [==============================] - 0s 608us/step\n",
      "coom_round: 69 | global_acc: 98.117% | global_loss: 1.484067440032959\n",
      "188/188 [==============================] - 0s 605us/step\n",
      "coom_round: 70 | global_acc: 98.183% | global_loss: 1.4836740493774414\n",
      "188/188 [==============================] - 0s 610us/step\n",
      "coom_round: 71 | global_acc: 98.133% | global_loss: 1.4834344387054443\n",
      "188/188 [==============================] - 0s 627us/step\n",
      "coom_round: 72 | global_acc: 98.083% | global_loss: 1.4835573434829712\n",
      "188/188 [==============================] - 0s 614us/step\n",
      "coom_round: 73 | global_acc: 98.050% | global_loss: 1.4836188554763794\n",
      "188/188 [==============================] - 0s 620us/step\n",
      "coom_round: 74 | global_acc: 98.133% | global_loss: 1.483422875404358\n",
      "188/188 [==============================] - 0s 608us/step\n",
      "coom_round: 75 | global_acc: 98.067% | global_loss: 1.4834190607070923\n",
      "188/188 [==============================] - 0s 662us/step\n",
      "coom_round: 76 | global_acc: 98.150% | global_loss: 1.483402132987976\n",
      "188/188 [==============================] - 0s 698us/step\n",
      "coom_round: 77 | global_acc: 98.117% | global_loss: 1.4833619594573975\n",
      "188/188 [==============================] - 0s 606us/step\n",
      "coom_round: 78 | global_acc: 98.167% | global_loss: 1.4829721450805664\n",
      "188/188 [==============================] - 0s 611us/step\n",
      "coom_round: 79 | global_acc: 98.050% | global_loss: 1.4831054210662842\n",
      "188/188 [==============================] - 0s 600us/step\n",
      "coom_round: 80 | global_acc: 98.150% | global_loss: 1.4831933975219727\n",
      "188/188 [==============================] - 0s 615us/step\n",
      "coom_round: 81 | global_acc: 98.183% | global_loss: 1.4831336736679077\n",
      "188/188 [==============================] - 0s 638us/step\n",
      "coom_round: 82 | global_acc: 98.117% | global_loss: 1.4830031394958496\n",
      "188/188 [==============================] - 0s 652us/step\n",
      "coom_round: 83 | global_acc: 98.067% | global_loss: 1.483057975769043\n",
      "188/188 [==============================] - 0s 608us/step\n",
      "coom_round: 84 | global_acc: 98.083% | global_loss: 1.4831982851028442\n",
      "188/188 [==============================] - 0s 598us/step\n",
      "coom_round: 85 | global_acc: 98.100% | global_loss: 1.4832210540771484\n",
      "188/188 [==============================] - 0s 669us/step\n",
      "coom_round: 86 | global_acc: 98.050% | global_loss: 1.4833214282989502\n",
      "188/188 [==============================] - 0s 618us/step\n",
      "coom_round: 87 | global_acc: 98.083% | global_loss: 1.482953667640686\n",
      "188/188 [==============================] - 0s 618us/step\n",
      "coom_round: 88 | global_acc: 98.067% | global_loss: 1.4831297397613525\n",
      "188/188 [==============================] - 0s 618us/step\n",
      "coom_round: 89 | global_acc: 98.100% | global_loss: 1.4831690788269043\n",
      "188/188 [==============================] - 0s 650us/step\n",
      "coom_round: 90 | global_acc: 98.100% | global_loss: 1.4830009937286377\n",
      "188/188 [==============================] - 0s 675us/step\n",
      "coom_round: 91 | global_acc: 98.033% | global_loss: 1.4829554557800293\n",
      "188/188 [==============================] - 0s 603us/step\n",
      "coom_round: 92 | global_acc: 98.117% | global_loss: 1.482557773590088\n",
      "188/188 [==============================] - 0s 612us/step\n",
      "coom_round: 93 | global_acc: 98.133% | global_loss: 1.4826396703720093\n",
      "188/188 [==============================] - 0s 603us/step\n",
      "coom_round: 94 | global_acc: 98.150% | global_loss: 1.4827734231948853\n",
      "188/188 [==============================] - 0s 611us/step\n",
      "coom_round: 95 | global_acc: 98.100% | global_loss: 1.4828095436096191\n",
      "188/188 [==============================] - 0s 624us/step\n",
      "coom_round: 96 | global_acc: 98.100% | global_loss: 1.4827908277511597\n",
      "188/188 [==============================] - 0s 625us/step\n",
      "coom_round: 97 | global_acc: 98.033% | global_loss: 1.4829610586166382\n",
      "188/188 [==============================] - 0s 673us/step\n",
      "coom_round: 98 | global_acc: 98.100% | global_loss: 1.4826124906539917\n",
      "188/188 [==============================] - 0s 610us/step\n",
      "coom_round: 99 | global_acc: 98.133% | global_loss: 1.4826686382293701\n"
     ]
    }
   ],
   "source": [
    "# Initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784,10)\n",
    "\n",
    "comms_round = 100\n",
    "\n",
    "# start global training loop\n",
    "for comm_round in range(comms_round):\n",
    "    #  get the global model's weights - initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "\n",
    "    # List to collect local model weights after scaling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    # Randomize client data using keys\n",
    "    client_names = list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "\n",
    "    # Loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784,10)\n",
    "        local_model.compile(loss=loss,\n",
    "                            optimizer=optimizer,\n",
    "                            metrics=metrics)\n",
    "        # set weights of the global model as weights of the local modelk\n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        # Fit local model with client data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "\n",
    "        # scale the model weights and add to the list\n",
    "        scaling_factor = weight_scaling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(),scaling_factor)\n",
    "        scaled_local_weightsmlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784,10)\n",
    "        local_model.compile(loss=loss,\n",
    "                            optimizer=optimizer,\n",
    "                            metrics=metrics)\n",
    "        # set weights of the global model as weights of the local modelk\n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        # Fit local model with client data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "\n",
    "        # scale the model weights and add to the list\n",
    "        scaling_factor = weight_scaling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(),scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "        # Clear session to clean memory\n",
    "        k.clear_session()\n",
    "\n",
    "    # update global model\n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    # Test globalmodel\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[<tf.Tensor: shape=(784, 200), dtype=float32, numpy=\\narray([[ 0.03571758, -0.02571049, -0.02710956, ...,  0.05763042,\\n         0.05246304,  0.06008835],\\n       [ 0.0049664 , -0.07706694, -0.00089854, ...,  0.07466882,\\n         0.06001882,  0.06352755],\\n       [-0.02584577,  0.07723933,  0.0243814 , ..., -0.05433658,\\n         0.00056902,  0.06095746],\\n       ...,\\n       [-0.00387093, -0.0413771 ,  0.05094896, ..., -0.03800261,\\n        -0.02351406, -0.05670906],\\n       [ 0.02975903, -0.00170064,  0.00195459, ..., -0.05663487,\\n        -0.0453493 , -0.04946703],\\n       [ 0.00289438,  0.06778017, -0.03451001, ...,  0.04838387,\\n         0.07487383,  0.06311972]], dtype=float32)>, <tf.Tensor: shape=(200,), dtype=float32, numpy=\\narray([ 0.02436729, -0.02040631, -0.04043369,  0.03196613,  0.04834822,\\n        0.02461036,  0.0206391 ,  0.10492345, -0.06469017,  0.03604339,\\n       -0.02722654, -0.01371731,  0.06116929, -0.01725947,  0.03431245,\\n       -0.09000932,  0.03107955, -0.0079158 ,  0.02870015,  0.07526509,\\n        0.03733671,  0.07755662, -0.00987103,  0.03621117, -0.10730545,\\n        0.00121967, -0.04661619,  0.02249148,  0.04877514,  0.0085578 ,\\n       -0.00254085,  0.03266349, -0.01813673,  0.14685565,  0.05223497,\\n        0.00281894, -0.01804678,  0.06421211, -0.10251566,  0.00634265,\\n        0.01030374,  0.04697438,  0.08361389,  0.06888692,  0.02773739,\\n       -0.01957624,  0.02895135,  0.09050467,  0.07905968,  0.08394855,\\n        0.06790647, -0.01075263,  0.02494367,  0.04744893,  0.00038406,\\n       -0.05995392, -0.02809746,  0.06463931,  0.03346876, -0.02980041,\\n       -0.02696739, -0.07921638,  0.09156252, -0.01458363,  0.04842554,\\n        0.02552671, -0.0668062 ,  0.08419835,  0.00147672,  0.06251403,\\n        0.08163121,  0.10813672,  0.05362672,  0.02563314, -0.00917632,\\n        0.08086085,  0.0698007 ,  0.1317212 , -0.01165527,  0.01320122,\\n       -0.00670672,  0.06291006,  0.06944365,  0.0642216 ,  0.00909034,\\n        0.04011951,  0.06227721, -0.01245577,  0.02410741,  0.00326026,\\n        0.03253967, -0.03265431,  0.14743678,  0.11304693, -0.00866384,\\n        0.0157311 ,  0.00781988, -0.02658558,  0.07817558,  0.12748848,\\n        0.06499541, -0.00613764,  0.01433802, -0.0066488 , -0.04091898,\\n        0.03840302, -0.03268597,  0.06196817, -0.04886446,  0.05666246,\\n        0.09709077, -0.05501886,  0.08352595,  0.02519068, -0.03950379,\\n        0.0267564 ,  0.08022982,  0.00567519,  0.07513825,  0.01838594,\\n        0.12912829,  0.03328703, -0.09525429,  0.0690659 ,  0.03808679,\\n        0.03536163,  0.00877286,  0.06048303,  0.03075619,  0.01591647,\\n        0.0344076 , -0.06983913, -0.03585659,  0.05006891,  0.08621469,\\n        0.07685624,  0.05433953,  0.00819187,  0.02483653,  0.03097268,\\n        0.03886402,  0.12150739,  0.02073028,  0.02482998,  0.01909826,\\n        0.11522634,  0.01229976,  0.00462364,  0.11037827, -0.00905765,\\n       -0.04228018, -0.03129206,  0.03753229,  0.0413594 ,  0.09360792,\\n        0.04741999,  0.00056353, -0.00763857,  0.0439995 ,  0.08077653,\\n       -0.1004254 ,  0.00511785, -0.00662511,  0.03449394,  0.03848151,\\n        0.04108308, -0.02076428, -0.00166108,  0.08427352,  0.16150232,\\n       -0.02191414,  0.01465565,  0.08481829, -0.08468132,  0.01778927,\\n        0.01289964,  0.12219192, -0.00429447, -0.026314  , -0.00420213,\\n        0.08190341, -0.01155089,  0.08199455,  0.11931593,  0.0671619 ,\\n        0.01173454,  0.08518129,  0.01968688, -0.07357023, -0.05652411,\\n        0.00877067, -0.0872943 , -0.11786076,  0.06865187, -0.03662764,\\n       -0.03321026,  0.00644447,  0.10267274, -0.05909324, -0.01744089],\\n      dtype=float32)>, <tf.Tensor: shape=(200, 200), dtype=float32, numpy=\\narray([[-0.06553791, -0.09370743, -0.05344853, ..., -0.07827515,\\n         0.06230985,  0.01935549],\\n       [ 0.09317457,  0.02921438,  0.05602928, ...,  0.15897663,\\n        -0.14650299,  0.03659612],\\n       [-0.0709487 ,  0.05797606, -0.06800682, ..., -0.12937908,\\n         0.12125362,  0.05720159],\\n       ...,\\n       [-0.14995556,  0.03022296,  0.06100432, ...,  0.03301438,\\n        -0.05918055, -0.0515724 ],\\n       [-0.1190695 , -0.06637703,  0.03311692, ..., -0.07426417,\\n         0.08511795,  0.00829031],\\n       [-0.12409228, -0.04275786,  0.07079336, ...,  0.07706957,\\n         0.07636968,  0.02814816]], dtype=float32)>, <tf.Tensor: shape=(200,), dtype=float32, numpy=\\narray([ 2.6902124e-02,  6.6007033e-02,  5.5698510e-03,  1.7080862e-02,\\n        6.8244904e-02,  2.0832134e-02,  9.0567648e-02,  3.6944475e-03,\\n        7.3949918e-02, -1.2856761e-03, -7.1253028e-04, -3.1293519e-02,\\n        3.9819516e-02, -5.3853109e-05, -7.9058688e-03,  5.0749902e-02,\\n        3.6514740e-02, -5.6574501e-02, -1.2581726e-02,  1.4778706e-02,\\n        1.8657573e-02,  9.5878460e-02,  5.8502767e-02,  3.1225506e-02,\\n        5.7762686e-02, -6.4729657e-03,  1.9904893e-02,  9.6952975e-02,\\n        6.6771612e-02,  2.4463855e-02,  2.2904247e-03,  1.3972378e-01,\\n        4.5757432e-02,  4.4428720e-03, -2.0293592e-02,  2.4087874e-02,\\n       -1.4553310e-02,  2.1508913e-02,  8.8606596e-02,  1.0017218e-01,\\n       -3.5400384e-03,  1.4624713e-01,  7.1236700e-02,  7.0234977e-02,\\n        4.6759982e-02,  2.0727009e-02, -2.6494250e-02,  6.8269740e-03,\\n        4.9926467e-02,  4.4370074e-02,  5.3317863e-02,  3.4840390e-02,\\n        9.9651121e-02,  1.1664399e-02,  2.0602074e-02,  8.3658099e-04,\\n        2.6059674e-02,  3.0628923e-02,  3.0196056e-02, -1.1562561e-02,\\n        1.1158922e-02,  2.6706805e-02,  4.1952245e-02,  4.2857736e-02,\\n        1.6555648e-03,  2.2366203e-02, -4.2254832e-03,  7.4320741e-02,\\n       -1.6228735e-02, -1.3486378e-02,  7.6056972e-02,  8.3535694e-02,\\n       -5.4009706e-03,  3.0621348e-02, -2.5887014e-02,  2.8340658e-02,\\n       -3.1905588e-02,  1.2745713e-01,  9.5088318e-02,  2.4460830e-02,\\n        5.1057439e-02, -8.4651439e-03,  6.1124224e-02,  1.3146781e-02,\\n        5.3260848e-03,  5.4081082e-03,  6.9491334e-02,  1.6103234e-02,\\n        8.7235346e-03,  8.0197165e-03,  1.1504961e-01, -6.4889039e-03,\\n        8.6172476e-02,  6.9507420e-02,  2.2596049e-03,  7.6442333e-03,\\n       -1.3135274e-02,  3.6466595e-02, -1.1817715e-02,  1.4868971e-02,\\n        3.3618085e-02,  3.8618669e-02,  5.5471215e-02,  3.6381096e-02,\\n       -1.0294717e-02, -3.5258709e-03,  3.2404665e-02, -5.1128971e-03,\\n       -2.2421721e-02,  1.1586633e-02,  1.1547604e-01,  7.2320579e-03,\\n        2.7963292e-02,  2.7387071e-02,  2.4621012e-02,  4.5251995e-02,\\n        5.9697654e-02,  4.1437256e-03, -3.3447361e-03,  3.5850175e-02,\\n       -2.3545608e-02,  5.7386611e-02,  7.3417559e-02,  1.3000219e-02,\\n       -7.9554915e-03,  3.3390727e-02,  1.4620034e-02,  2.4455843e-02,\\n        1.3664249e-01, -5.5178567e-03,  4.0566299e-02,  1.0334810e-02,\\n        8.2136015e-04, -1.6860800e-02,  4.6568844e-02, -1.3198265e-02,\\n        1.9483451e-03,  5.2923560e-03,  3.3566192e-02,  4.3166127e-02,\\n        1.4046117e-02, -7.1346345e-03,  3.7514862e-02,  2.9599506e-02,\\n       -6.7399587e-03,  2.9750267e-02,  1.0138730e-02,  5.2956339e-02,\\n        1.1275837e-01,  4.8818842e-02,  1.1836269e-01, -7.1306811e-03,\\n       -1.4089846e-02, -1.2435075e-03, -1.0854490e-02,  6.2946049e-03,\\n        2.7925620e-02, -1.3567972e-02, -1.3850050e-02,  1.8883822e-03,\\n        6.3011631e-02,  1.2198277e-03,  4.4345958e-03,  5.6778423e-02,\\n        3.2850504e-02,  2.3778884e-02,  5.9619870e-02,  4.2939384e-02,\\n        1.0643865e-02,  4.8056193e-02,  7.2793603e-02, -2.2992883e-03,\\n        8.8180592e-03, -5.5978920e-02,  4.7633823e-02, -7.7397854e-04,\\n        1.9251378e-02,  3.4359034e-02,  2.4033308e-02,  4.1159920e-02,\\n        1.4523598e-02,  1.3543433e-02,  2.2461386e-02, -3.5747206e-03,\\n       -9.4382595e-03,  7.4884035e-02, -1.6809286e-03,  8.0458105e-02,\\n        3.6796227e-02,  2.1153444e-02,  3.1660151e-02,  1.1596849e-01,\\n       -4.1662697e-03, -1.8798303e-02, -5.3725280e-02,  6.1699495e-02,\\n        6.7943357e-02,  8.4995860e-03,  6.6165291e-02,  1.0235049e-02],\\n      dtype=float32)>, <tf.Tensor: shape=(200, 10), dtype=float32, numpy=\\narray([[ 0.15184018,  0.3178872 ,  0.24296674, ..., -0.16199875,\\n        -0.19264764,  0.17029452],\\n       [ 0.14327458,  0.22374722,  0.29399282, ..., -0.37419584,\\n        -0.32926705,  0.14358586],\\n       [-0.08030761,  0.12318928,  0.00133686, ..., -0.04765342,\\n         0.09364907, -0.16149516],\\n       ...,\\n       [-0.34579098, -0.01517641,  0.39199397, ...,  0.29171845,\\n        -0.16079918, -0.20984136],\\n       [-0.17615171,  0.06877927, -0.44531453, ...,  0.2914727 ,\\n         0.24355336,  0.34845114],\\n       [ 0.08095343, -0.11464714, -0.11941482, ...,  0.1480755 ,\\n         0.00231134, -0.1555538 ]], dtype=float32)>, <tf.Tensor: shape=(10,), dtype=float32, numpy=\\narray([-0.15387805,  0.09339742, -0.02496138, -0.08453155,  0.0423071 ,\\n        0.1126956 , -0.06261349, -0.07116979,  0.0369949 ,  0.11175261],\\n      dtype=float32)>]'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(average_weights) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export architecture to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_string = global_model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 784], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"batch_input_shape\": [null, 784], \"units\": 200, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation\", \"trainable\": true, \"dtype\": \"float32\", \"activation\": \"relu\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 200, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation_1\", \"trainable\": true, \"dtype\": \"float32\", \"activation\": \"relu\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 10, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation_2\", \"trainable\": true, \"dtype\": \"float32\", \"activation\": \"softmax\"}}]}, \"keras_version\": \"2.11.0\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export weights as h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.save_weights('weights.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export weights as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(average_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b07f451ebbbe1f80453345214d70880b5111d33133c65ac28c98b3e601695cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
